# -*- coding: utf-8 -*-
"""lstm-project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e55Lnl0I0gFgzrbOwEAGsqmnKKwAWpRO
"""

# read lines from file
def read_lines(file):
    with open(file, 'r') as f:
        lines = f.read()
    return lines

faqs = read_lines('phone-key_dev.txt')

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer()

tokenizer.fit_on_texts([faqs])

tokenizer_json = tokenizer.to_json()
with open('tokenizer_config.json', 'w', encoding='utf-8') as f:
    f.write(tokenizer_json)

# prompt: download the generated json file

# from google.colab import files
# save the tokenizer json file
with open('tokenizer_config.json', 'w', encoding='utf-8') as f:
    f.write(tokenizer_json)

input_sequences = []
for sentence in faqs.split('\n'):
  tokenized_sentence = tokenizer.texts_to_sequences([sentence])[0]

  for i in range(1,len(tokenized_sentence)):
    input_sequences.append(tokenized_sentence[:i+1])

total_words = len(tokenizer.word_index) + 1
max_len = max([len(x) for x in input_sequences])

print(max_len)

from tensorflow.keras.preprocessing.sequence import pad_sequences
padded_input_sequences = pad_sequences(input_sequences, maxlen = max_len, padding='pre')

padded_input_sequences

X = padded_input_sequences[:,:-1]

y = padded_input_sequences[:,-1]

from tensorflow.keras.utils import to_categorical
y = to_categorical(y,num_classes=total_words)

y.shape

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

model = Sequential()
model.add(Embedding(total_words, 100, input_length=max_len-1))
model.add(LSTM(150))
model.add(Dense(total_words, activation='softmax'))

# model = Sequential()
# model.add(Embedding(31123, 100, input_length=103))
# model.add(LSTM(50))
# model.add(Dense(31123, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])

model.summary()

model.fit(X,y,epochs=100)

model.save('model.h5')

# load the model
# from tensorflow.keras.models import load_model
# model = load_model('model.h5')

import time
import numpy as np
text = "Thank you for"


# tokenize
token_text = tokenizer.texts_to_sequences([text])[0]
# padding
padded_token_text = pad_sequences([token_text], maxlen=70, padding='pre')
# predict
prediction = model.predict(padded_token_text)
pos = np.argmax(prediction)

for word,index in tokenizer.word_index.items():
  if index == pos:
    # text = text + " " + word
    print(text + " " + word)

padded_token_text

pos

# from google.colab import files
# prompt: convert the generated model to tflite and download it

# Convert the model to TFLite
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,  # Enable TensorFlow Lite ops.
    tf.lite.OpsSet.SELECT_TF_OPS  # Enable TensorFlow ops.
]
tflite_model = converter.convert()

# Save the TFLite model
open("converted_model.tflite", "wb").write(tflite_model)